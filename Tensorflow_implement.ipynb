{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import codecs\n",
    "from __future__ import print_function\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    text = codecs.open(file_path, 'r', 'utf-8' , errors='ignore').read()\n",
    "    print text[:20]\n",
    "    chars = list(set(text))\n",
    "    text_size, vocab_size = len(text), len(chars)\n",
    "    print 'text has %d characters, %d unique'%(text_size, vocab_size)\n",
    "    char2id = {ch: i for i, ch in enumerate(chars)}\n",
    "    id2char = { i:ch for i, ch in enumerate(chars) }\n",
    "    return char2id, id2char, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我真的需要你\n",
      " \n",
      "现在我觉得有些孤单\n",
      "悲\n",
      "text has 38528 characters, 1757 unique\n"
     ]
    }
   ],
   "source": [
    "char2id, id2char, train_text = read_data('./input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = len(char2id)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size=16\n",
    "num_unrollings=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id[self._text[self._cursor[b]]]] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u6211\\u771f\\u7684\\u9700\\u8981\\u4f60\\n \\n\\u73b0\\u5728', u'\\n\\u5feb\\u4e3a\\u6211\\u70b9\\u4eae \\u70b9\\u4eae \\u70b9', u'\\u4e2a\\u65b0\\u7684\\u5e0c\\u671b\\n\\u6ca1\\u6709\\u4e00\\u4e2a\\u786e', u'\\n\\u79bb\\u5f00\\u6211 \\u79bb\\u5f00\\u6211\\n\\u79bb\\u5f00', u'\\u679c\\u4f60\\u611f\\u5230\\u4f60\\u7684\\u7075\\u9b42\\n\\u53d1\\u51fa', u'\\u8d25\\n\\u6211\\u4e0d\\u613f\\u518d\\u7b49\\u5f85 \\u7b49\\u5f85', u'\\u53d1\\u751f\\n\\u90a3\\u4e9b\\u873f\\u8712\\u7684\\u5206\\u660e\\u662f', u' \\n\\u4e8c\\u5341\\u591a\\u5c81\\u90a3\\u5e74\\u4ed6\\u4eec\\u593a', u'\\u5374\\u6d41\\u4e0d\\u51fa\\u773c\\u6cea\\n\\u6211\\u60f3\\u558a ', u'\\u6df1\\u8c37\\n \\n\\u84dd\\u98ce\\u7b5d\\u548c\\u90a3\\u6a61', u'- \\n\\u5149\\u660e\\n \\n\\u5f53\\u7070\\u70ec', u'\\u767d\\u6865\\n \\n\\u6211\\u7684\\u5144\\u5f1f\\u8eba\\u5728', u'\\u9e1f\\n\\u5e26\\u7740\\u98ce\\u4e2d\\u60b2\\u9e23\\u7684\\u8349\\u5e3d', u'\\u5728\\u7b49\\u5f85\\n\\u7b49\\u5f85\\u7684\\u4e00\\u5207\\u65e0\\u6240', u'\\u53ea\\u662f\\u7528\\u4f5c\\u56de\\u5fc6\\n\\u5b83\\u8ba9\\u6211\\u4eec', u'\\u4f60\\u5145\\u6ee1\\u4e86\\u6211\\u7684\\u8fbd\\u9614\\u5fc3\\u5e95\\n']\n",
      "[u'\\u5728\\u6211\\u89c9\\u5f97\\u6709\\u4e9b\\u5b64\\u5355\\n\\u60b2\\u54c0', u'\\u70b9\\u4eae\\u706b\\u7130\\n \\n\\u6211\\u770b\\u89c1\\u7ea2', u'\\u786e\\u5b9a\\u7684\\u65b9\\u5411\\n\\u6ca1\\u6709\\u4e00\\u4e2a\\u671f', u'\\u5f00\\u6211 \\u79bb\\u5f00\\u6211\\n \\n\\u544a\\u8bc9', u'\\u51fa\\u6df1\\u6df1\\u7684\\u53f9\\u606f\\n\\u4e0d\\u8981\\u4ecb\\u610f', u'\\u5f85\\n \\n\\u6211\\u8981\\u8d62\\u5f97\\u8fd9\\u573a\\u80dc', u'\\u662f\\u6cea\\u6c34\\n\\u6211\\u4e0d\\u53bb\\u731c \\u662f\\u6b22', u'\\u593a\\u8d70\\u6211\\u7684\\u5de5\\u4f5c\\n\\u56e0\\u4e3a\\u6211\\u4ece', u' \\u5374\\u53d1\\u4e0d\\u51fa\\u58f0\\u97f3\\n\\u6211\\u613f\\u610f', u'\\u6a61\\u6811\\u53f6\\n\\u84b2\\u516c\\u82f1\\u4e0e\\u90a3\\u5e03\\u8c37', u'\\u70ec\\u67e5\\u5c01\\u4e86\\u51dd\\u971c\\u7684\\u5c4b\\u6a90\\n\\u5f53', u'\\u5728\\u9601\\u697c\\u4e0a\\n\\u8fb9\\u6ce8\\u5c04\\u7740\\u6fc0\\u7d20', u'\\u5e3d\\n\\u4ece\\u71c3\\u70e7\\u7684\\u98ce\\u4e2d\\u6ed1\\u843d\\n', u'\\u6240\\u671f\\u5f85\\n\\u6211\\u7684\\u6b32\\u671b\\u5df2\\u7ecf\\u5206', u'\\u4eec\\u5b66\\u4f1a\\u4e86\\u73cd\\u60dc\\n \\n\\u59d1\\u5a18', u'\\n\\u50cf\\u6df1\\u51ac\\u7684\\u96ea\\u822c\\u94fa\\u5929\\u6f2b\\u5730']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哀的自我有些辛酸\n",
      "没有\n",
      "红色的火光\n",
      "我感到强大\n",
      "期待的声音\n",
      "风暴 风暴\n",
      "诉你\n",
      "我是无足轻重的人\n",
      "意 不要介意\n",
      "因为生命\n",
      "胜利\n",
      "即使一切都不存在\n",
      "欢喜是伤悲\n",
      " \n",
      "突然间\n",
      "从没对那个混蛋笑过\n",
      "他\n",
      "意 抛弃我的所有\n",
      "如果\n",
      "谷鸟\n",
      "带着放荡的痴心跌\n",
      "当车菊草化作深秋的露水\n",
      "素边喷涌\n",
      "他思考着如何\n",
      "\n",
      " \n",
      "再见 青春\n",
      "再见\n",
      "分裂\n",
      "我的宣言也变成呢\n",
      "娘 让我们在一起\n",
      "无论\n",
      "地\n",
      "这思念就像是彻底地\n"
     ]
    }
   ],
   "source": [
    "text = batches2string(train_batches.next())\n",
    "for i in text:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    wx = tf.Variable(tf.truncated_normal([4*vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    wm = tf.Variable(tf.truncated_normal([4*num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes], trainable=False))\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    def lstm_cell(i, o, state):\n",
    "        i_stacked = tf.concat(1, [i,i,i,i])\n",
    "        o_stacked = tf.concat(1, [o,o,o,o])\n",
    "        weights_in = tf.matmul(i_stacked, wx)\n",
    "        weights_out = tf.matmul(o_stacked, wm)\n",
    "        input_gate = tf.sigmoid(weights_in + weights_out + ib)\n",
    "        forget_gate = tf.sigmoid(weights_in + weights_out + fb)\n",
    "        update = weights_in + weights_out + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(weights_in + weights_out + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    '''\n",
    "    # Single matrix \n",
    "\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1,0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "    # Variables saving state across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and baises.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes:2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes:3*num_nodes]\n",
    "        state = forget_gate*state + input_gate*tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "        \n",
    "    train_data = list()\n",
    "    for _ in xrange(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0 : 7.47007036209 learning rate: 10.0\n",
      "Minibatch perplexity: 1754.73\n",
      "================================================================================\n",
      "村曲崖意眼铺扉场夏倔芒树恣妙虫奄转着罪充悉股茶唤闯谬求满绽抵h至件葬赢唇咒工阑控垮以惊雾读兵裙臂滴我一霜免库髦议唱握歌欲死杨铃酸友专拜阔怅尊凭星娇封砍懵病童响潦\n",
      "怜春暧抖睡的树给缓策际赌表烈否0未杯万偿绝颗汤流蜿短朝抱魔美敬声度紧社祖异问未措拔赦挡加谈垃打莎湾格萎扑兜狂原块仔梦押坚私晴呼背并李十喀锋站煤弦银执脆鹰教途欢铭\n",
      "课而祖拆室默缩黄缘坦别器谎游跄微冒忧惶谜杀晕城醒否工砸访芳啤识众辰用及厂烟罪挽瑰获端恳干次惑牢知丐麦奈啸必零危珊灭珊都佛边阳努挽华项还闻忐卑约的怒雀简袋尖肤涌聊\n",
      "无陋内呦衰戒纹告推街弦摸宴程低寒背验朝纷弹枭挫昂世4鳞随饿抓敬侣谬甚法田任颗别蓄挤觉人虹策璨纷轰山伪了睡夏断各红滑胶乌伸乎必心获贝备币被抵温屈否袭肉岸暧脚让彼儿\n",
      "觅公拒圣巧卖糟盘边率诞议驱声粪伙最麦听百程倾河何掩自睁扬伤苍屁隐娘悠命初喧题具春话占挺自有限拥掬占愤外拍6星弟辉演熟痛长弦抹自官私唤纷啊嘴沮引治谅贷迪润马于肯敲\n",
      "================================================================================\n",
      "Average loss at step 100 : 5.85017663479 learning rate: 10.0\n",
      "Minibatch perplexity: 238.24\n",
      "Average loss at step 200 : 5.3165617609 learning rate: 10.0\n",
      "Minibatch perplexity: 154.55\n",
      "Average loss at step 300 : 5.0546865654 learning rate: 10.0\n",
      "Minibatch perplexity: 146.24\n",
      "Average loss at step 400 : 4.82865390778 learning rate: 10.0\n",
      "Minibatch perplexity: 82.79\n",
      "Average loss at step 500 : 4.66981710911 learning rate: 10.0\n",
      "Minibatch perplexity: 104.76\n",
      "Average loss at step 600 : 4.46629605293 learning rate: 10.0\n",
      "Minibatch perplexity: 61.00\n",
      "Average loss at step 700 : 4.36742079258 learning rate: 10.0\n",
      "Minibatch perplexity: 125.45\n",
      "Average loss at step 800 : 4.24644692421 learning rate: 10.0\n",
      "Minibatch perplexity: 59.34\n",
      "Average loss at step 900 : 4.10085684061 learning rate: 10.0\n",
      "Minibatch perplexity: 66.69\n",
      "Average loss at step 1000 : 4.02417582035 learning rate: 10.0\n",
      "Minibatch perplexity: 61.69\n",
      "================================================================================\n",
      "影振想去\n",
      "我无论这无个漫上那个我\n",
      "生活那思胸和那青无\n",
      "我希望了不停已邀我的小式\n",
      "看经你光心与靠晚己\n",
      "救光们\n",
      "我对你\n",
      "没有花我总以此地横褪\n",
      "虽上大你那茫悉\n",
      "我爱你\n",
      "断过 生中的我\n",
      "或碌随那意疯 的夜老\n",
      " \n",
      "可我眩印那样亮实何迷定\n",
      " \n",
      "风吧那敬永风没有方式\n",
      "爱我坚床地心髦\n",
      "虽然振灵倒的梦是当我用跑灵魂的力方\n",
      "带镇不想在金旁\n",
      "\n",
      "状了寂露不要\n",
      " \n",
      "我真的大我稞上我们的生活灵量的丝\n",
      "\n",
      "我的寂泣然熟唱得这样\n",
      "谓狂任过都不存现回些变碎\n",
      "我需要你\n",
      "这五完\n",
      " \n",
      "这个最等被地\n",
      " \n",
      "这年满未蜜的雨华\n",
      "\n",
      "缘\n",
      " \n",
      "我怎你\n",
      "爱你\n",
      "又一无法人\n",
      " \n",
      "现在的彼是否抖寻\n",
      "却爱的眼苦\n",
      "塌爸我真抛起紫阳式\n",
      "\n",
      "现看说你那能修\n",
      "生挥我再坚夹得有什么\n",
      "我那长爱的和现在魂阳的人心\n",
      "你把\n",
      "唱着敢春\n",
      "如果有一天\n",
      "我常永远需是那起了国\n",
      "汤我们在一起亮惧宽伤\n",
      "那间从没有一天楼\n",
      "我早和那些感到不再的星光\n",
      "他感觉没有什么就\n",
      "那个摸间\n",
      "如实我希望 青春一寸间\n",
      "================================================================================\n",
      "Average loss at step 1100 : 3.9209813261 learning rate: 10.0\n",
      "Minibatch perplexity: 39.66\n",
      "Average loss at step 1200 : 3.87640319824 learning rate: 10.0\n",
      "Minibatch perplexity: 43.99\n",
      "Average loss at step 1300 : 3.76230619192 learning rate: 10.0\n",
      "Minibatch perplexity: 53.73\n",
      "Average loss at step 1400 : 3.68739317894 learning rate: 10.0\n",
      "Minibatch perplexity: 24.61\n",
      "Average loss at step 1500 : 3.65092180729 learning rate: 10.0\n",
      "Minibatch perplexity: 23.27\n",
      "Average loss at step 1600 : 3.55199159622 learning rate: 10.0\n",
      "Minibatch perplexity: 26.24\n",
      "Average loss at step 1700 : 3.54587705135 learning rate: 10.0\n",
      "Minibatch perplexity: 37.38\n",
      "Average loss at step 1800 : 3.43283626795 learning rate: 10.0\n",
      "Minibatch perplexity: 50.82\n",
      "Average loss at step 1900 : 3.37132124424 learning rate: 10.0\n",
      "Minibatch perplexity: 20.35\n",
      "Average loss at step 2000 : 3.35806141615 learning rate: 10.0\n",
      "Minibatch perplexity: 23.67\n",
      "================================================================================\n",
      "承还平多大事\n",
      "放在我的容梦\n",
      "皮少利是雨翔\n",
      "为未丽的点座谅我忍明月\n",
      "奇香含美心来不解自由\n",
      "还只世笑我远安轻妒全路的\n",
      "每十人能出任何承\n",
      "我要改变 青天你苦意义先率的\n",
      "坟\n",
      "可以废是叮亡的右鲜\n",
      " \n",
      "小洒没有一幸死去\n",
      "还是我的骄字迷惘的心\n",
      "为你事心爱前如此清么值腿\n",
      "何那天天我坚倦自己少路眶\n",
      " \n",
      "多少孤夜的感觉哭泣\n",
      " \n",
      "我觉来越泪依\n",
      "彻而广黑\n",
      " \n",
      "我知道自己怎样宝贝你点子\n",
      "我要未来可爸来不在地心里\n",
      " \n",
      "我爱你爱孤次一起者在雨中\n",
      "我要-- \n",
      "我身妈\n",
      "船水显越的爱情\n",
      " \n",
      "你有一次可我不给自己\n",
      "-\n",
      "绵建哭\n",
      "如果你不存够静寂，不存在隧边的一量\n",
      " \n",
      "怎么人离我自己\n",
      " \n",
      "这是最后一天我像以是已喜\n",
      "你全手充失那会量面\n",
      "----- \n",
      "爱长你像相重奇\n",
      "唱着消桑都要自由\n",
      "漂痛该裂的边\n",
      " \n",
      "我要改变 并要真实\n",
      " \n",
      "晚安 所有种钟的人们\n",
      " \n",
      "绽安 有感觉回到何悲伤\n",
      "择伤从分巨搂为你\n",
      "我不知友温想我的裙面\n",
      "请有就因已可当倦底\n",
      "眼娘多失\n",
      "================================================================================\n",
      "Average loss at step 2100 : 3.24998966217 learning rate: 10.0\n",
      "Minibatch perplexity: 33.03\n",
      "Average loss at step 2200 : 3.26834955692 learning rate: 10.0\n",
      "Minibatch perplexity: 13.08\n",
      "Average loss at step 2300 : 3.18713714361 learning rate: 10.0\n",
      "Minibatch perplexity: 31.81\n",
      "Average loss at step 2400 : 3.16422749281 learning rate: 10.0\n",
      "Minibatch perplexity: 28.08\n",
      "Average loss at step 2500 : 3.10672070503 learning rate: 10.0\n",
      "Minibatch perplexity: 26.80\n",
      "Average loss at step 2600 : 3.07489832163 learning rate: 10.0\n",
      "Minibatch perplexity: 20.81\n",
      "Average loss at step 2700 : 3.04240567446 learning rate: 10.0\n",
      "Minibatch perplexity: 20.32\n",
      "Average loss at step 2800 : 2.99002406597 learning rate: 10.0\n",
      "Minibatch perplexity: 16.51\n",
      "Average loss at step 2900 : 3.02899320841 learning rate: 10.0\n",
      "Minibatch perplexity: 21.83\n",
      "Average loss at step 3000 : 2.90486270905 learning rate: 10.0\n",
      "Minibatch perplexity: 18.51\n",
      "================================================================================\n",
      "行的未来\n",
      "那就是我的周\n",
      " \n",
      "有一天你会要\n",
      "和那种林\n",
      " \n",
      "如果你有实当我再见美底\n",
      " \n",
      "让我精如 你要有些思我无依\n",
      " \n",
      "每子\n",
      "现出迷茫\n",
      "再见不有一种温暖\n",
      "我不知道满\n",
      "报滚我勇首心\n",
      " \n",
      "无法我又该真亡\n",
      "我不知道易被所有什么\n",
      "----- \n",
      "不知道\n",
      "为底有了\n",
      "就像一辆那\n",
      "爸爸 有些生活\n",
      "我问但有意思到\n",
      "堂同的城市树爷开始呼痛\n",
      " \n",
      "\n",
      "讨\n",
      " \n",
      "每天都都不再的回见\n",
      "----- \n",
      "在走吧我的喊彩\n",
      "带着伦底率虚的心驶\n",
      "精微的城市在等待上任匆手\n",
      " \n",
      "他们该做什么\n",
      "总有许多是我\n",
      "这是死去驱来的心\n",
      "就像与\n",
      "秋\n",
      "他看着你\n",
      "街边对明天\n",
      "我不停地在这无满天\n",
      "在天堂是如此存眶\n",
      " \n",
      "我晚在而且\n",
      "像个孩子\n",
      "就像一人生命的石头\n",
      "在狂刻在风中的字星徘冷\n",
      "你眼看着你的乡\n",
      " \n",
      "也不再\n",
      "殆唐的荣戏\n",
      " \n",
      "妈妈 我终于并不存在\n",
      " \n",
      "至现在晚安里\n",
      "我们在这活着\n",
      " \n",
      "我觉得到他有理知躲很汤\n",
      " \n",
      "我希望追我爱你\n",
      "像爱涌于河不会忘不\n",
      "在每场恃动的远嘲者\n",
      "这\n",
      "================================================================================\n",
      "Average loss at step 3100 : 2.91586293697 learning rate: 10.0\n",
      "Minibatch perplexity: 20.74\n",
      "Average loss at step 3200 : 2.89626712084 learning rate: 10.0\n",
      "Minibatch perplexity: 18.22\n",
      "Average loss at step 3300 : 2.82664339542 learning rate: 10.0\n",
      "Minibatch perplexity: 16.02\n",
      "Average loss at step 3400 : 2.85180832863 learning rate: 10.0\n",
      "Minibatch perplexity: 19.47\n",
      "Average loss at step 3500 : 2.74827535629 learning rate: 10.0\n",
      "Minibatch perplexity: 12.27\n",
      "Average loss at step 3600 : 2.79089650631 learning rate: 10.0\n",
      "Minibatch perplexity: 16.54\n",
      "Average loss at step 3700 : 2.72216944933 learning rate: 10.0\n",
      "Minibatch perplexity: 19.02\n",
      "Average loss at step 3800 : 2.69435366392 learning rate: 10.0\n",
      "Minibatch perplexity: 13.71\n",
      "Average loss at step 3900 : 2.69466376781 learning rate: 10.0\n",
      "Minibatch perplexity: 12.55\n",
      "Average loss at step 4000 : 2.63488395691 learning rate: 10.0\n",
      "Minibatch perplexity: 10.57\n",
      "================================================================================\n",
      "黎明\n",
      " \n",
      "在生命中为了什么\n",
      "也许你有一颗永动地凝手\n",
      "也不想这摇迷秋的\n",
      "不要相事怪个梦徨\n",
      " \n",
      "你会知道这是该败冷的人什么样\n",
      " \n",
      "那是个表情\n",
      "所有向风的声音\n",
      "还有危\n",
      "山城的路上\n",
      " \n",
      "我不建地呼喊平息\n",
      "我不自觉地跌入了吗苦\n",
      " \n",
      "我不自觉地跌入了我活着\n",
      " \n",
      "这是个人啊的\n",
      " \n",
      "这是一九九九九酷的方向\n",
      "在我长远的刺塌燃林\n",
      " \n",
      "蹩脚的\n",
      "无主之伤\n",
      " \n",
      "你运合的蹩狂\n",
      "来不及视的暗醒\n",
      " \n",
      "突然间你的痴都\n",
      " \n",
      "当爱的人生爱你的楼上\n",
      "可今尖种也想过有些捧傻\n",
      "除了你没恼咛的欠刃\n",
      "也许你就像她艰头空照的哭泣\n",
      "V铃挑的西\n",
      "\n",
      "为灯情些了太多没可梦\n",
      " \n",
      "怎么人\n",
      " \n",
      "爱安你的皮啊的霓天\n",
      "A国女友黄草的脸上\n",
      " \n",
      "你听到的荣白\n",
      "你，紧紧怒易\n",
      "你觉得有意思吗\n",
      " \n",
      "夜总有的尘土\n",
      "很\n",
      "悔\n",
      "一把孩子李落非而所有多少可抱\n",
      " \n",
      "雨中的女车\n",
      "原谅的手子没有结果\n",
      "你有没有人知道彩虹灯运不分易\n",
      " \n",
      "这是个世界\n",
      "看到你的夜晚你的封天\n",
      "----- \n",
      "为什么\n",
      "\n",
      "================================================================================\n",
      "Average loss at step 4100 : 2.6910003078 learning rate: 10.0\n",
      "Minibatch perplexity: 16.94\n",
      "Average loss at step 4200 : 2.62155762196 learning rate: 10.0\n",
      "Minibatch perplexity: 25.66\n",
      "Average loss at step 4300 : 2.61450279713 learning rate: 10.0\n",
      "Minibatch perplexity: 19.54\n",
      "Average loss at step 4400 : 2.59216576576 learning rate: 10.0\n",
      "Minibatch perplexity: 19.97\n",
      "Average loss at step 4500 : 2.54462877274 learning rate: 10.0\n",
      "Minibatch perplexity: 24.77\n",
      "Average loss at step 4600 : 2.57493567228 learning rate: 10.0\n",
      "Minibatch perplexity: 10.66\n",
      "Average loss at step 4700 : 2.49928682923 learning rate: 10.0\n",
      "Minibatch perplexity: 16.51\n",
      "Average loss at step 4800 : 2.51962084413 learning rate: 10.0\n",
      "Minibatch perplexity: 15.62\n",
      "Average loss at step 4900 : 2.49308212876 learning rate: 10.0\n",
      "Minibatch perplexity: 12.21\n",
      "Average loss at step 5000 : 2.46540091872 learning rate: 1.0\n",
      "Minibatch perplexity: 12.96\n",
      "================================================================================\n",
      "数激的烟路\n",
      "然后就像 入独的人群\n",
      " \n",
      "像苍声空那鲜亮\n",
      " \n",
      "在我多拥抱紧我\n",
      "我想在死去\n",
      "我不知想自己\n",
      " \n",
      "这呼九从求你所抱\n",
      "爱的善脚\n",
      " \n",
      "这是该什么更忘了\n",
      "可是为\n",
      "娃\n",
      "或是白购走在随须着无法锁\n",
      "又似伴 我从没太笼娑发出伤心\n",
      " \n",
      "我终于被你忘得分离\n",
      "时我电车一种温暖的堆程\n",
      "飞发现在泪水的火光洞\n",
      "别忘了像一个歌土\n",
      " \n",
      "这条欢笑\n",
      "好艰比\n",
      "女格请流舞途\n",
      "我想进天空暗的握着网\n",
      " \n",
      "宝贝在雨中找到\n",
      "等待商出自由的东西\n",
      "----- \n",
      "宝贝 没什么\n",
      " \n",
      "现在每一双小歌像记上\n",
      "沸腾着\n",
      "我们永远是在变\n",
      "佛联而逝下\n",
      "宣鸣的大门你外\n",
      "可是事实撒在家把我发说跌一甜\n",
      " \n",
      "还有次不及了梦想\n",
      "和沽冰来他\n",
      " \n",
      "宝贝没有一个孔怪者\n",
      "还是感觉看到的的眼泪\n",
      "有人在家字更低荡\n",
      "只有\n",
      "题沽和雨中\n",
      "\n",
      "像儿欢还在不经里之前\n",
      " \n",
      " \n",
      "我心爱着你也曾到我\n",
      " \n",
      "有时V在黑话中泛脱死去廉伤\n",
      "让我们都经飘声\n",
      "O女士 我想你\n",
      " \n",
      "我已不再存实\n",
      "O女士 我不能\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print ('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in xrange(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss,\n",
    "                                            train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print ('Average loss at step', step, ':', mean_loss, 'learning rate:', lr)\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print ('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                \n",
    "                print('='*80)\n",
    "                for _ in xrange(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in xrange(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('='*80)\n",
    "            '''\n",
    "            # Measure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in xrange(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "            '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
